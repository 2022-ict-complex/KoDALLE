{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to generate Image with our dall-e model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch\n",
    "\n",
    "import torch\n",
    "\n",
    "from einops import repeat\n",
    "\n",
    "# vision imports\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# dalle related classes and utils\n",
    "\n",
    "from dalle_pytorch import VQGanVAE\n",
    "from dalle_pytorch.tokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성하고싶은 이미지의 설명\n",
    "raw_text = \"아우터는 색상이 브라운 소재가 우븐 핏이 노멀인 재킷이다. 하의는 색상이 블랙 소재가 우븐 핏이 노멀인 팬츠이다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "DALLE_PATH = './dalle.pt'\n",
    "\n",
    "DALLE_CFG = EasyDict()\n",
    "\n",
    "# argument parsing\n",
    "\n",
    "DALLE_CFG.VQGAN_PATH = \"./VQGAN_blue_e7\"   # './vae.pt' - will use OpenAIs pretrained VAE if not set\n",
    "DALLE_CFG.VQGAN_CFG_PATH = \"./VQGAN_blue.yaml\"   # './vae.pt' - will use OpenAIs pretrained VAE if not set\n",
    "DALLE_CFG.DALLE_PATH = \"./dalle.pt\"   # './vae.pt' - will use OpenAIs pretrained VAE if not set\n",
    "\n",
    "DALLE_CFG.WPE_PATH = \"./roberta_large_wpe.pt\"\n",
    "DALLE_CFG.WTE_PATH = \"./roberta_large_wte.pt\"\n",
    "\n",
    "# DALLE_CFG.MODEL_DIM = 512\n",
    "DALLE_CFG.TEXT_SEQ_LEN = 128\n",
    "\n",
    "# Top-k level\n",
    "DALLE_CFG.TOP_K=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "\n",
    "encoded_dict = tokenizer(\n",
    "    raw_text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=DALLE_CFG.TEXT_SEQ_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=True,  # for RoBERTa\n",
    ")\n",
    "encoded_dict=encoded_dict.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dalle_pytorch import VQGanVAE\n",
    "from pathlib import Path\n",
    "\n",
    "DALLE_CFG.VOCAB_SIZE = tokenizer.vocab_size # refer to EDA, there are only 333 words total. but input_ids index should be in within 0 ~ 52000: https://github.com/boostcampaitech2-happyface/DALLE-Couture/blob/pytorch-dalle/EDA.ipynb\n",
    "\n",
    "\n",
    "loaded_obj = torch.load(DALLE_PATH, map_location=torch.device('cuda'))\n",
    "\n",
    "dalle_params, _ , weights = loaded_obj['hparams'], loaded_obj['vae_params'], loaded_obj['weights']\n",
    "\n",
    "vae_klass = VQGanVAE\n",
    "vae = vae_klass(\n",
    "    vqgan_model_path=DALLE_CFG.VQGAN_PATH, \n",
    "    vqgan_config_path=DALLE_CFG.VQGAN_CFG_PATH\n",
    "    )\n",
    "\n",
    "DALLE_CFG.IMAGE_SIZE = vae.image_size\n",
    "\n",
    "dalle_params = dict(        \n",
    "    **dalle_params\n",
    ")\n",
    "\n",
    "DALLE_CFG.IMAGE_SIZE = vae.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DALLE_Klue_Roberta\n",
    "from clip.clipmodel import *\n",
    "\n",
    "dalle = DALLE_Klue_Roberta(\n",
    "    vae = vae, \n",
    "    wte_dir=DALLE_CFG.WTE_PATH,\n",
    "    wpe_dir=DALLE_CFG.WPE_PATH,\n",
    "    **dalle_params\n",
    "    )\n",
    "dalle.load_state_dict(weights)\n",
    "dalle.to('cuda')\n",
    "#encoded_dict=repeat(encoded_dict,'() n -> b n',b=DALLE_CFG.TOP_K)\n",
    "# https://github.com/lucidrains/DALLE-pytorch/blob/main/dalle_pytorch/dalle_pytorch.py#L454-L510\n",
    "clip_model = torch.load(\"clip.pt\",map_location=torch.device('cuda'))\n",
    "clip_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dalle.generate_images(encoded_dict,clip=clip_model,img_num=DALLE_CFG.TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid, save_image\n",
    "from PIL import Image\n",
    "lgits=images[1]\n",
    "probs = torch.nn.Softmax(dim=1)(lgits)\n",
    "import matplotlib.pyplot as plt\n",
    "print(f'Input Text: {raw_text}')\n",
    "### Sorting 통해 text랑 image사이 거리가 가장 가까운 순으로 출력 ### \n",
    "for idx, prob in sorted(enumerate(probs[0]),key = lambda x: x[1], reverse = True):\n",
    "    print(f'probability: {prob.item()}')\n",
    "    grid = make_grid(images[0][idx], nrow=1, padding=0, pad_value=0)\n",
    "    ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
